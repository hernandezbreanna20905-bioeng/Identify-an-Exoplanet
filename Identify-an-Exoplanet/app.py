# -*- coding: utf-8 -*-
"""app

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UdcBi684xa_B3wUqGD20ytMuYOiNFSCV
"""

# app.py
# Streamlit app for Kepler tabular ML (Random Forest + sigma/IQR cleaning)
import io
import numpy as np
import pandas as pd
import streamlit as st
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay

st.set_page_config(page_title="Kepler Exoplanet Identifier", page_icon="ðŸª", layout="wide")
st.title("ðŸª Kepler Exoplanet Identifier")

st.info("Upload Kepler KOI tabular data, clean outliers (sigma/IQR), train a Random Forest, and predict disposition.")

# -------------------------
# Utility & defaults
# -------------------------
DEFAULT_FEATURES = [
    # Transit geometry
    "koi_period", "koi_duration", "koi_depth", "koi_prad",
]

TARGET_COL = "koi_disposition"
TARGET_MAP = {"FALSE POSITIVE": 0, "CANDIDATE": 1, "CONFIRMED": 2}
TARGET_NAMES = ["False Positive", "Candidate", "Confirmed"]

def remove_outliers_sigma(df, columns, sigma=3.0):
    cleaned = df.copy()
    for col in columns:
        if col not in cleaned.columns:
            continue
        mean = cleaned[col].mean()
        std = cleaned[col].std()
        if std and not np.isnan(std) and std > 0:
            z = (cleaned[col] - mean) / std
            cleaned = cleaned[np.abs(z) <= sigma]
    return cleaned

def remove_outliers_iqr(df, columns, factor=1.5):
    cleaned = df.copy()
    for col in columns:
        if col not in cleaned.columns:
            continue
        Q1 = cleaned[col].quantile(0.25)
        Q3 = cleaned[col].quantile(0.75)
        IQR = Q3 - Q1
        lower = Q1 - factor * IQR
        upper = Q3 + factor * IQR
        mask = (cleaned[col] >= lower) & (cleaned[col] <= upper)
        cleaned = cleaned[mask]
    return cleaned

# -------------------------
# Data input
# -------------------------
with st.expander("ðŸ“¥ Data"):
    st.write("Upload your `kepler_data.csv` (KOI table). If your file has header notes, set `skiprows` accordingly.")
    uploaded = st.file_uploader("Upload CSV", type=["csv"])
    skiprows = st.number_input("skiprows (for header notes)", min_value=0, value=53, step=1)
    df = None

    if uploaded is not None:
        df = pd.read_csv(uploaded, skiprows=skiprows)
        st.write("**Raw data (head)**")
        st.dataframe(df.head())
    else:
        st.warning("No CSV uploaded yet.")


feature_cols = []
if df is not None:
    feature_cols = [c for c in DEFAULT_FEATURES if c in df.columns]

# Sidebar controls
with st.sidebar:
    st.header("âš™ï¸ Settings")
    # Cleaning
    st.subheader("Outliers")
    use_sigma = st.checkbox("Sigma filter (z-score)", value=True)
    sigma_val = st.slider("Â± Sigma threshold", 2.0, 6.0, 3.0, 0.5)
    use_iqr = st.checkbox("IQR filter", value=True)
    iqr_factor = st.slider("IQR factor", 1.0, 4.0, 1.5, 0.5)

    # Split & model
    st.subheader("Train/Test split")
    test_size = st.slider("Test size", 0.1, 0.4, 0.2, 0.05)
    random_state = st.number_input("random_state", value=42, step=1)

    st.subheader("Random Forest")
    n_estimators = st.slider("n_estimators", 50, 1000, 300, 50)
    max_depth = st.slider("max_depth (0 = None)", 0, 50, 12, 1)
    class_weight_balanced = st.checkbox("class_weight='balanced'", value=True)

# -------------------------
# Data preparation & model
# -------------------------
if df is not None and feature_cols and TARGET_COL in df.columns: #feature_col=list of column names
    with st.expander("ðŸ§¹ Data preparation"):
        # Clean column names
        df.columns = df.columns.str.strip()

        # Keep only rows with all selected features and target present (no NaNs)
        st.write("**Drop rows with NaNs in selected features/target**")
        before = len(df)
        df = df.dropna(subset=feature_cols + [TARGET_COL])
        st.write(f"Rows before: {before} â†’ after dropna: {len(df)}")

        # Outlier filtering
        df_clean = df.copy()
        if use_sigma:
            before_sigma = len(df_clean)
            df_clean = remove_outliers_sigma(df_clean, feature_cols, sigma=sigma_val)
            st.write(f"Sigma filter (Â±{sigma_val}Ïƒ): {before_sigma} â†’ {len(df_clean)} rows")

        if use_iqr:
            before_iqr = len(df_clean)
            df_clean = remove_outliers_iqr(df_clean, feature_cols, factor=iqr_factor)
            st.write(f"IQR filter (Ã—{iqr_factor}): {before_iqr} â†’ {len(df_clean)} rows")

        st.write("**Cleaned data (head)**")
        st.dataframe(df_clean.head())

        # X, y
        X = df_clean[feature_cols].copy()
        y = df_clean[TARGET_COL].map(TARGET_MAP)

        # Scaling (standardize numeric features)
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)

        # Split
        X_train, X_test, y_train, y_test = train_test_split(
            X_scaled, y, test_size=test_size, stratify=y, random_state=random_state
        )

        st.write("**Shapes**")
        st.code({
            "X_train": X_train.shape, "X_test": X_test.shape,
            "y_train": y_train.shape, "y_test": y_test.shape
        })

    with st.expander("ðŸ§  Train model & evaluate", expanded=True):
        params = dict(
            n_estimators=n_estimators,
            random_state=random_state
        )
        if max_depth > 0:
            params["max_depth"] = max_depth
        if class_weight_balanced:
            params["class_weight"] = "balanced"

        clf = RandomForestClassifier(**params)
        clf.fit(X_train, y_train)

        test_acc = clf.score(X_test, y_test)
        train_acc = clf.score(X_train, y_train)

        st.metric("Test Accuracy", f"{test_acc:.3f}", delta=f"{(test_acc-train_acc):+.3f} vs train")
        st.caption(f"Train accuracy: **{train_acc:.3f}**")

        # Detailed report
        y_pred = clf.predict(X_test)
        report = classification_report(y_test, y_pred, target_names=TARGET_NAMES, output_dict=True)  ###shows models performance
        st.write("**Classification report**")
        st.dataframe(pd.DataFrame(report).T)

    # -------------------------
    # Inference UI (single prediction)
    # -------------------------
    with st.sidebar:
        st.header("ðŸ”® Single-sample prediction")
        st.caption("Enter values or pick an existing row to preview a prediction.")

        mode = st.radio("Input mode", ["Manual", "Pick from data"])
        if mode == "Pick from data":
            idx = st.number_input("Row index (from cleaned data)", min_value=0, max_value=len(df_clean)-1, value=0, step=1)
            sample = df_clean.iloc[[idx]][feature_cols]
            st.write("Picked row values")
            st.dataframe(sample)
        else:
            # Manual inputs with sensible defaults from column stats
            fills = {}
            for c in feature_cols:
                colmin = float(df_clean[c].min())
                colmax = float(df_clean[c].max())
                colmean = float(df_clean[c].mean())
                # slider requires min<max; fallback if constant column
                if colmin == colmax:
                    colmin = colmin - 1.0
                    colmax = colmax + 1.0
                fills[c] = st.slider(f"{c}", colmin, colmax, float(colmean))
            sample = pd.DataFrame([fills], columns=feature_cols)

        # Scale and predict
        sample_scaled = scaler.transform(sample)
        proba = clf.predict_proba(sample_scaled)[0]   ###models confidence
        pred_class = np.argmax(proba)
        st.subheader("Prediction")
        st.write(dict(zip(TARGET_NAMES, map(lambda x: round(float(x), 4), proba))))
        st.success(f"Predicted: **{TARGET_NAMES[pred_class]}**")
else:
    st.stop()
